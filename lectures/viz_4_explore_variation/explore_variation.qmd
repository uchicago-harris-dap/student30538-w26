---
title: "Visualization (Exploring variation)"
author: "Peter Ganong and Maggie Shi"
date: January 25, 2026
date-format: long
format: 
  revealjs:
    slide-number: true
    show-slide-number: all
    self-contained: true
    code-overflow: wrap
---

```{python fig-align="center"}
#| echo: false
import altair as alt
from vega_datasets import data
import plotnine
#from plotnine import *
from plotnine.data import diamonds, mpg
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import tempfile 
from IPython.display import SVG, display
import vl_convert as vlc
url = ("https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv")
penguins = pd.read_csv(url)
alt.data_transformers.enable('default', max_rows=None);

## helper functions  for displaying altair
def display_altair_svg(chart):
    """
    Render an Altair chart to SVG and display it inline.
    """
    svg_text = vlc.vegalite_to_svg(chart.to_dict())

    with tempfile.NamedTemporaryFile(suffix=".svg", mode="w") as tmp:
        tmp.write(svg_text)
        tmp.flush()
        display(SVG(filename=tmp.name))

def display_altair_png(chart, scale=2):
    """
    Render an Altair chart to a PNG and display it inline.

    Parameters
    ----------
    chart : altair.Chart
        Altair chart object to render.
    scale : int, optional
        Resolution scaling factor for the PNG (default = 2). Use scale=2 for standard slides. Use scale = 3–4 for dense figures or PDF exports.
    """
    png_bytes = vlc.vegalite_to_png(chart.to_dict(), scale=scale)

    # Write to a temporary PNG file and display
    with tempfile.NamedTemporaryFile(suffix=".png") as tmp:
        tmp.write(png_bytes)
        tmp.flush()
        display(Image(filename=tmp.name))

## define axis font sizes in altair
def axis_theme():
    return {
        "config": {
            "axis": {
                "labelFontSize": 18,
                "titleFontSize": 20
            }
        }
    }


_ = alt.themes.register("axis_theme", axis_theme) #assign to dummy variable _ so it doesn't make a print statement
_ = alt.themes.enable("axis_theme")
```



# Introduction

## Skills hopefully acquired at the end of lecture

Take a single variable in a dataset. 

Visualize to learn more about it. Key cases of interest:
* Categorical variables (in lecture 2 we called these `N`/`O`)
* Continuous variables (in lecture 2 we called these `Q`)
    * Exploring typical values
    * Exploring and dealing with unusual values

(Up next class: covariation with two variables)

## Roadmap of lecture {style="font-size: 0.9em"}
* Introduce three key ideas in data visualization
    * exploration vs. production
    * headline vs. sub-messages 
    * iteration
* introduce datasets for this lecture
* Categorical variables (in lecture 2 we called these `N`/`O`)
* Continuous variables (in lecture 2 we called these `Q`)
    * Exploring typical values
    * Exploring and dealing with unusual values

## What is exploratory data analysis?

Data visualization has two distinct goals

1. **exploration** for you to *learn* as much as possible
2. **production** for you to *teach* someone else what you think the key lessons are

## Exploration vs. production

* When you are in exploration mode, you will look at lots of patterns and your brain filters out the noise
* Production mode is like putting a cone on your dog. 
* You are deliberately limiting the reader's field of vision such that they see the key messages from the plot *and avoid too many distractions*


## "A Sunday on La Grande Jatte" by Seurat

![](pictures/seurat.jpg)

::: {.notes }
**NOTES:**
*for those of you who have not seen the painting (and perhaps some of you who have seen it), you noticed that it's actually made up of millions of little dots. This is a style known as pointillism. Sort of like pixels before we had computer screens. It's a good example of how you can notice more detail and more interesting detail by looking closely at a painting or a plot.

In addition, there's something else (flip back) that you notice by looking closely which is that none of the people in the painting are looking at each other. They are all in their own worlds, isolated from and not interacting with one another. Obviously this is not an art course, our main point is just that there's huge value to looking closely at the plots that you make*
:::


## "A Sunday on La Grande Jatte" by Seurat
![](pictures/seurat_zoomed.jpg){width=80%}

## Headline vs submessages {style="font-size: 0.9em"}

* **Headline** is what you see first when you look at the painting or you look at the plot.
* **Submessages** are what you see later, on closer inspection
* Often, the most interesting patterns in the data are ones that you don't see right away when you make the very first plot. 


## Iterating on plot design



> "Make dozens of plots"

Quoctrung Bui, former 30538 guest lecturer and former Harris data viz instructor

## Iterating on plot design

What does he mean?

* The first plot you make will never be the one you should show
* As you are generating graphs for yourself in exploration mode, you will produce many candidates that could end up being used in production mode
* As a rule of thumb, you should try out at least three different plotting concepts (marks)
* Within each concept, you will need to try out several different encodings

## Summary:
1. Decide if you are trying to explore the data or produce a plot for someone else
2. For any given plot, look closely (like Seurat) beyond just the headline
3. Iterate

# Intro to data
## Introduction to data  {style="font-size: 0.7em"}

* Most of our visualization lectures are based on the [University of Washington](https://idl.uw.edu/visualization-curriculum/) textbook, but the textbook doesn't have enough material on exploratory data analysis. So we are supplementing with
    * [Data Visualization](https://r4ds.hadley.nz/data-visualize)
    * [Exploratory Data Analysis](https://r4ds.hadley.nz/EDA.html) material in the **R for Data Science** textbook (with the code translated to Altair)

* `diamonds`, `mpg` are from "Exploratory Data Analysis"
* `movies` (also used last lecture) is from the UW textbook 
* `penguins` is from "Data Visualization" 

# Categorical variables

## Categorical variables: roadmap 

* introduce `diamonds`
* show table
* show bar graph

## introduce dataset `diamonds` {style="font-size: 0.9em"}
<div style="font-size: 90%">

```{python}
#| echo: true 
import pandas as pd
from plotnine.data import diamonds
diamonds.shape
```

```{python}
#| echo: true 
diamonds.head()
```

</div>



## `diamonds` data dictionary
<div style="font-size: 60%">

Variable | Definition | Values |
| --- | --- | --- | 
`price`| price in USD | $326–$18,823 |
`carat`| weight of diamond | 0.2-5.01 |
`cut` | quality of the cut | Fair, Good, Very Good, Premium, Ideal |
`color` | diamond color | D (best) to J (worst) |
`clarity` | measure of how clear diamond is | I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best) |
`x` | length in mm | 0-10.74 |
`y` | width in mm | 0-58.9 |
`z` | depth in mm | 0-31.8 |
`depth` | $\frac{z}{ mean(x, y)}$ | 43-79 |
`table` | width of top of diamond relative to widest point  | 43-95 |


</div>



##  Summarizing `cut` in a table
```{.python}
diamonds_cut = diamonds.groupby('cut').size()
diamonds_cut
```

```{python}
#| warning: false
diamonds_cut = diamonds.groupby('cut').size()
diamonds_cut
```

## Summarizing with a bar graph (code)
```{.python code-line-numbers="1-2|4-5,8|"} 
#moves the index (`cut`) into a normal column so Altair can read it.
diamonds_cut = diamonds_cut.reset_index().rename(columns={0:'N diamonds'})

# extract ordering of `cut` as a Python list to use for sorting
cut_order = diamonds['cut'].cat.categories.tolist()

alt.Chart(diamonds_cut).mark_bar().encode(
    alt.X('cut:O', title = "Cut", sort=cut_order),
    alt.Y('N diamonds:Q', title = "Count")
).properties(width=640, height=360).configure_axis(
    labelFontSize=18,
    titleFontSize=20
)
```

**Note**: we have included syntax to modify graph properties here. Going forward our `.qmd` source code uses these throughout, but we will omit in the slides for the sake of space.

## Summarizing with a bar graph (plot)
```{python fig-align="center"}
#moves the index (`cut`) into a normal column so Altair can read it.
diamonds_cut = diamonds_cut.reset_index().rename(columns={0:'N diamonds'})
cut_order = diamonds['cut'].cat.categories.tolist()

chart = alt.Chart(diamonds_cut).mark_bar().encode(
    alt.X('cut:O', title = "Cut", sort=cut_order),
    alt.Y('N diamonds:Q', title = "Count")
).properties(width=900, height=450)

display_altair_svg(chart)
```


## Categorical variables: summary

* This section is very brief because there's basically only one good way to plot categorical variables with a small number of categories and this is it. 
    * You can use `mark_point()` instead of `mark_bar()`, but overall, there's a clear right answer about how to do this. 
* We include this material mainly to foreshadow the fact that we will do a lot on categorical variables in the next lecture when we get to "Exploring Co-variation"


# Continuous Variables

## Continuous variables: roadmap

* Binning + histograms using `movies`
* Histograms and density plots using `penguins`
* Exploring carat size using `diamonds`

Remark: The skills are absolutely fundamental and so we will intentionally be a bit repetitive.


## Recall: `movies` dataset

<div style="font-size: 50%">
```{.python}
movies_url = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'
movies = pd.read_json(movies_url)
movies.head()
```



```{python}
movies_url = 'https://cdn.jsdelivr.net/npm/vega-datasets@1/data/movies.json'
movies = pd.read_json(movies_url)
movies.head()
```
</div>

## Histogram using `mark_bar()`
* **Rotten Tomatoes** ratings are determined by taking "thumbs up" and "thumbs down" judgments from film critics and calculating the percentage of positive reviews.
* This is a continuous measure, but we can bin it to create a **histogram** of frequencies

## Histogram using `mark_bar()`
```{.python code-line-numbers="2|2-3|"}
hist_rt = alt.Chart(movies_url).mark_bar().encode(
    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20), title = "Rotten Tomatoes Rating (%)"),
    alt.Y('count():Q', title = "Count")
)
hist_rt
```

```{python}
#| fig-align: center
hist_rt = alt.Chart(movies_url).mark_bar().encode(
    alt.X('Rotten_Tomatoes_Rating:Q', bin=alt.BinParams(maxbins=20), title = "Rotten Tomatoes Rating (%)"),
    alt.Y('count():Q', title = "Count")
).properties(
    width=800,
    height=360
)
# have to do configure_axis here instead of when defining hist_rt so that we can do the horizontal concatenation later (hist_rt | hist_imdb)

display_altair_svg(hist_rt)
```

Discussion question: what are the headline and sub-messages?

::: {.notes }
**NOTES:**
*solution: 
Headline: distribution is close to uniform.
Sub-messages
1) the bottom bin out of 20 has very few movies 
2) there's a gentle shift/trend upward so higher-rated movies are a bit more common than lower-rated movies*
:::


## Histogram of IMDB ratings
**IMDB ratings** are formed by averaging scores (ranging from 1 to 10) provided by the site's users.
```{.python}
hist_imdb = alt.Chart(movies_url).mark_bar().encode(
    alt.X('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20), title = "IMDB Ratings"),
    alt.Y('count():Q', title = "Count")
)
hist_imdb
```

```{python}
#| fig-align: center
hist_imdb = alt.Chart(movies_url).mark_bar().encode(
    alt.X('IMDB_Rating:Q', bin=alt.BinParams(maxbins=20), title = "IMDB Ratings"),
    alt.Y('count():Q', title = "Count")
).properties(
    width=800,
    height=360
)

display_altair_svg(hist_imdb)
```

## Side-by-side {style="font-size: 0.8em"}
```{.python}
hist_rt | hist_imdb
```

```{python}
#| fig-align: center
side_by_side = hist_rt | hist_imdb
display_altair_svg(side_by_side)
```

Discussion question: compare the two ratings distributions. If _your goal for the headline of the graph is about differentiating between good and bad movies_, which rating is more informative?

::: {.notes }
**NOTES:**
*solution: Rotten tomatoes. IMDB is too compressed.*
:::

## Introducing the `penguins` dataset

```{.python}
url = ("https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv")
penguins = pd.read_csv(url)
penguins.head()
```

<div style="font-size: 70%">
```{python}
url = ("https://raw.githubusercontent.com/mcnakhaee/palmerpenguins/master/palmerpenguins/data/penguins.csv")
penguins = pd.read_csv(url)
penguins.head()
```
</div>

## Histogram with steps of 200
* We previously picked the maximum number of equally-spaced bins (`BinParams(maxbins=20)`) and let `altair` choose "nice"-looking bin widths for the histogram
* Alternatively, we can manually control the bin width using `step`

## Histogram with steps of 200
```{.python code-line-numbers="2|"}
alt.Chart(penguins).mark_bar().encode(
    alt.X('body_mass_g:Q', bin=alt.BinParams(step=200), title = "Body Mass (g)"),
    alt.Y('count():Q', title = "Count")
)
```

```{python}
#| fig-align: center
chart = alt.Chart(penguins).mark_bar().encode(
    alt.X('body_mass_g:Q', bin=alt.BinParams(step=200), title = "Body Mass (g)"),
    alt.Y('count():Q', title = "Count")
).properties(
    width=800, 
    height=400
)


display_altair_svg(chart)
```

## Histogram `step` parameter

`step=20` vs. `step=200` vs, `step=2000` 

```{python}
#| echo: false
#| fig-align: center

plot1 = alt.Chart(penguins).mark_bar().encode(
    alt.X('body_mass_g', bin=alt.BinParams(step=20), title = "Body Mass (g)"),
    alt.Y('count()', title = "Count")).properties(
    width=400, 
    height=400
)

plot2 = alt.Chart(penguins).mark_bar().encode(
    alt.X('body_mass_g', bin=alt.BinParams(step=200), title = "Body Mass (g)"),
    alt.Y('count()', title = "Count")).properties(
    width=400, 
    height=400
)

plot3 = alt.Chart(penguins).mark_bar().encode(
    alt.X('body_mass_g', bin=alt.BinParams(step=2000), title = "Body Mass (g)"),
    alt.Y('count()', title = "Count")).properties(
    width=400, 
    height=400
)

combined = (plot1 | plot2 | plot3).configure_axis(
    labelFontSize=18,
    titleFontSize=20
)


display_altair_svg(combined)

```


Discussion question: what headline message(s) come from each `step` choice? Which do you prefer?


::: {.notes }
**NOTES:**
*solution 
the middle plot is by far the easiest to interpret. The top plot shows the bins in such a thin way that it is hard to see much of anything. 

FYI, this deliberately intended to be a setup for the diamonds density plot coming up in the next subsection because there adding narrower bins is actually quite revealing*
:::

## Density plot {style="font-size: 0.8em"}
An alternative to a histogram for exploring frequency in continuous variable: density plot using `transform_density`

```{.python code-line-numbers="1-3|"}
alt.Chart(penguins).transform_density(
    'body_mass_g',
    as_=['body_mass_g2', 'density']
).mark_area().encode(
    alt.X('body_mass_g2:Q', title = "Body Mass (g)"),
    alt.Y('density:Q', title = "Density")
)
```

```{python}
#| fig-align: center
chart = alt.Chart(penguins).transform_density(
    'body_mass_g',
    as_=['body_mass_g2', 'density']
).mark_area().encode(
    alt.X('body_mass_g2:Q', title = "Body Mass (g)"),
    alt.Y('density:Q', title = "Density")
).properties(
    width=1000, 
    height=400
)

display_altair_svg(chart)
```
::: {.notes }
**NOTES:**
Explaining the syntax:
transform_density('body_mass_g', ...) tells altair to compute a kernel density estimate
as_=['body_mass_g2', 'density'] names the output of the density transform -- first item (body_mass_g2) is name of x-values and second is the name for the y-values (density)
:::
    
## Back to diamonds, focus on `carat` {style="font-size: 0.8em"}
```{.python code-line-numbers="1|"}
alt.data_transformers.disable_max_rows()  #disable 5k max rows

alt.Chart(diamonds).mark_bar().encode(
    alt.X('carat', bin=alt.Bin(maxbins=10), title = "Carat"),
    alt.Y('count()', title = "Count")
)
``` 

```{python}
#| fig-align: center
# Define bin edges
bin_edges = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5]

# Bin 'carat' and extract left/right bin edges
diamonds_forplotting = diamonds
diamonds_forplotting['carat_bin'] = pd.cut(diamonds_forplotting['carat'], bins=bin_edges, right=False)
diamonds_forplotting['carat_bin_left'] = diamonds_forplotting['carat_bin'].apply(lambda x: x.left)
diamonds_forplotting['carat_bin_right'] = diamonds_forplotting['carat_bin'].apply(lambda x: x.right)

# Count rows per bin
binned = diamonds_forplotting.groupby(['carat_bin_left', 'carat_bin_right']).size().reset_index(name='count')

# Plot using rects with white stroke
chart = alt.Chart(binned).mark_rect(stroke='white', strokeWidth=1).encode(
    x=alt.X('carat_bin_left:Q', title='Carat', axis=alt.Axis(format=".1f")),
    x2='carat_bin_right:Q',
    y=alt.Y('count:Q', title='Count')
).properties(
    width=800, 
    height=400
)

display_altair_svg(chart)
```

First plot iteration reveals most of sample is < 2




::: {.notes }
**NOTES:**
<div style="font-size: 50%">
_Side note: disabling this actually does not work when compiling RevealJS (but it does work for PDFs and interactive), so the source code for this lecture uses `pandas` to bin before plotting_
</div>

(explanation from ChatGPT of the manual pandas override on this (and following) slide)
The first code chunk shows what you'd typically write in Jupyter or if you were compiling to PDF to make a histogram with Altair — just bin the `carat` variable using Altair's built-in `bin=alt.Bin(...)` and disable the row limit with `alt.data_transformers.disable_max_rows()`.

However, **this doesn't work when compiling to Revealjs**, because **Revealjs ignores that call** to `disable_max_rows()`. The chart is rendered at compile time, not interactively, so the row limit is still enforced behind the scenes, and the plot fails to render.

To work around that, we do the **binning manually in pandas**, and pass Altair a pre-aggregated summary table. This keeps the dataset under the 5000-row limit that Altair enforces for static rendering, making it fully Revealjs-compatible.

So the first code chunk is shown to illustrate what you might *want* to do in a notebook, but the second chunk is what's needed to make an exact replica that works with RevealjS.
:::
    


## Histogram of `carat` {style="font-size: 0.8em"}

```{.python code-line-numbers="1|"}
diamonds_small = diamonds.loc[diamonds['carat'] < 2.1]

alt.Chart(diamonds_small).mark_bar().encode(
    alt.X('carat', bin=alt.BinParams(step=0.2), title = "Carat"),
    alt.Y('count()', title = "Count")
)
```

```{python}
#| fig-align: center
# Step 1: Filter diamonds < 2.2 carats
diamonds_small = diamonds[diamonds['carat'] < 2.2].copy()

# Step 2: Use 0.2-width bins from 0.0 to 2.2
bin_edges = pd.interval_range(start=0.0, end=2.2, freq=0.2, closed='left')

# Step 3: Bin using pd.cut with include_lowest=True
diamonds_small['carat_bin'] = pd.cut(
    diamonds_small['carat'],
    bins=bin_edges,
    include_lowest=True
)

# Step 4: Count per bin
binned = diamonds_small['carat_bin'].value_counts(sort=False).reset_index()
binned.columns = ['carat_bin', 'count']
binned['carat_bin_left'] = binned['carat_bin'].apply(lambda x: x.left)
binned['carat_bin_right'] = binned['carat_bin'].apply(lambda x: x.right)

binned = binned.drop(columns=['carat_bin'])

# Step 5: Plot with tick values set manually to bin starts
chart = alt.Chart(binned).mark_rect(stroke='white', strokeWidth=0.5).encode(
   x=alt.X(
    'carat_bin_left:Q',
    title='Carat',
    scale=alt.Scale(domain=[0, 2.2]), 
    axis=alt.Axis(format=".2f", values=binned['carat_bin_left'].tolist(), grid = False)
    ),
    x2='carat_bin_right:Q',
    y=alt.Y('count:Q', title='Count')
).properties(
    width=800, 
    height=400
)

display_altair_svg(chart)
```

Second plot iteration reveals count is not _entirely_ decreasing in carat

## In-class exercise: histogram of `carat` {background-color="aliceblue" style="font-size: 0.8em"}

```{.python}
alt.Chart(diamonds_small).mark_bar().encode(
    alt.X('carat', bin=alt.BinParams(step=0.02), title = "Carat"),
    alt.Y('count()', title = "Count"))
```
```{python}
#| fig-align: center
# Step 1: Filter diamonds < 2.2 carats
diamonds_small = diamonds[diamonds['carat'] < 2.2].copy()

# Step 2: Use 0.2-width bins from 0.0 to 2.2
bin_edges = pd.interval_range(start=0.0, end=2.2, freq=0.02, closed='left')

# Step 3: Bin using pd.cut with include_lowest=True
diamonds_small['carat_bin'] = pd.cut(
    diamonds_small['carat'],
    bins=bin_edges,
    include_lowest=True
)

# Step 4: Count per bin
binned = diamonds_small['carat_bin'].value_counts(sort=False).reset_index()
binned.columns = ['carat_bin', 'count']
binned['carat_bin_left'] = binned['carat_bin'].apply(lambda x: x.left)
binned['carat_bin_right'] = binned['carat_bin'].apply(lambda x: x.right)

binned = binned.drop(columns=['carat_bin'])

# Step 5: Plot with tick values set manually to bin starts
chart = alt.Chart(binned).mark_rect(stroke='white', strokeWidth=0.5).encode(
   x=alt.X(
    'carat_bin_left:Q',
    title='Carat',
    scale=alt.Scale(domain=[0, 2.2]), 
    axis=alt.Axis(format=".2f", values=binned['carat_bin_left'].tolist(), grid = False)
    ),
    x2='carat_bin_right:Q',
    y=alt.Y('count:Q', title='Count')
).properties(
    width=1000, 
    height=300 )

chart
```


Discussion questions 

1. What is the headline of the 3rd plot iteration? Submessages?
2. What questions does it raise?

::: {.notes }
**NOTES:**
*
1. Headline: carats size does not seem to be evenly distributed, but instead follows discrete increments.
2. Are diamonds being cut down to exact sizes? Are diamond weights being fraudulently inflated? Why are some diamonds cut and not others?*

:::

## Typical continuous variables: summary 
* Main tool to explore uni-dimensional continuous variables: histograms
* Varying the bin widths can reveal different patterns

# Continuous variables: unusual values
<!-- source: Lecture 5, slides 26-44 -->
 
## Unusual continuous variables: roadmap

* case study: `y` dimension in diamonds
    * explore some unusual  values
    * three options for handling unusual values

## `diamonds`: identify unusual `y` values
First pass to examine for unusual values: summary statistics 
```{.python}
diamonds['y'].describe()
```

```{python}
diamonds = diamonds.drop(columns=["carat_bin_left", "carat_bin_right", "carat_bin"])
diamonds['y'].describe()
```

## `diamonds`: examine unusual `y` values


<div style="font-size: 70%">
```{python}
#| echo: true
diamonds.loc[(diamonds['y'] < 3) ] 
```
</div>

## `diamonds`: examine unusual `y` values

<div style="font-size: 70%">
```{python}
#| echo: true
diamonds.loc[(diamonds['y'] > 20)] 
```
</div>

## `diamonds`: compare to 10 random obs

<div style="font-size: 50%">
```{python}
#| echo: true
diamonds.sample(n=10)
```
</div>

## What to do with unusual values?

1. Drop row
2. Code value to `NA`
3. Winsorize value




## Option 1: drop rows
```{.python}
diamonds_clean = diamonds.loc[(diamonds['y'] >= 3) | (diamonds['y'] <= 20)] 
diamonds_clean
```

<div style="font-size: 50%">
```{python}
diamonds_clean = diamonds.loc[(diamonds['y'] >= 3) | (diamonds['y'] <= 20)] 
diamonds_clean
```
</div>

## Option 2: recode to missing


<div style="font-size: 50%">
```{python}
#| echo: true
diamonds_missing = diamonds.copy()
diamonds_missing['y'] = np.where((diamonds_missing['y'] < 3) |
     (diamonds_missing['y'] > 20), 
     np.nan, diamonds_missing['y'])
diamonds_missing[diamonds_missing['y'].isna()]
```
</div>

## Option 3: winsorize
Winsorizing re-codes outliers to a numeric value, keeping them in the data. 

To winsorize at 1 percent:

* Replace anything less than the 1st percentile with the 1st percentile
* Replace anything more than the 99th percentile with the 99th percentile

## Option 3: winsorize

```{.python}
diamonds_winsor = diamonds.copy()
pctile01 = diamonds_winsor['y'].quantile(0.01)
pctile99 = diamonds_winsor['y'].quantile(0.99)

print(f"1st Percentile: {pctile01}")
print(f"99th Percentile: {pctile99}")
```

```{python}
diamonds_winsor = diamonds.copy()
pctile01 = diamonds_winsor['y'].quantile(0.01)
pctile99 = diamonds_winsor['y'].quantile(0.99)

print(f"1st Percentile: {pctile01}")
print(f"99th Percentile: {pctile99}")
```


## Option 3: winsorize
```{.python}
diamonds_winsor['y_winsor'] = np.where(diamonds_winsor['y'] < pctile01, pctile01, 
                                np.where(diamonds_winsor['y'] > pctile99, pctile99, 
                                diamonds_winsor['y']))
diamonds_winsor
```


<div style="font-size: 50%">
```{python}
diamonds_winsor['y_winsor'] = np.where(diamonds_winsor['y'] < pctile01, pctile01, 
                                np.where(diamonds_winsor['y'] > pctile99, pctile99, diamonds_winsor['y']))
diamonds_winsor
```

</div>

## When might you winsorize?

An example from [Earnings Instability](https://bpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/1/801/files/2025/09/2025-08-20-earnings-instability.pdf) paper by Ganong and coauthors.

The paper is trying to quantify how much earnings change from month to month for the typical US worker. 

Consider the following fake data (next slide)

## Toy winsorization example

<div style="font-size: 60%">

:::: {.columns}
::: {.column width="79%"}

Suppose we have observations for earnings changes. 99\% of the data follows a normal distribution with std. dev. 0.2 and 1\% of the data is extremely large changes

| last month (\$) | this month (\$) | % change | \|% change \| | 
| --- | --- | --- | --- | 
| 600 |  600  |  0\% | 0\% 
| 600 | 570  |  -5\% | 5\% | 
|  600 | 540  |  -10\% | 10\% |
| 600 | 630  |  5\% | 5\% |
| ... |  | |
| (99\% of sample) |  | |
| ... |   |  |
| 600 | 300 | -50\% | 50\% | 
| 6000 | 300 | -95\% |  95\%  |
| 300 | 600 | 100\% | 100\% |  
| 300 | 6000 | 1900\% |  1900\% |  

:::
::: {.column width="19%"}
What is the standard deviation of the \% change in earnings?

| assumption | SD | 
| --- | --- | 
| do not winsorize | 97.2\% | 
| winsorize at 50\% | 20.5\%  |

Illustrative calculation [here](https://chatgpt.com/share/696a7266-2ecc-8008-a603-767109b02a2f)



When else is this useful? Income data, test scores, stock returns.

:::
::::



</div>

## Real-world winsorization example 

![](pictures/winsorization_example.png)

Source: Table A-2 from [Earnings Instability](https://bpb-us-w2.wpmucdn.com/voices.uchicago.edu/dist/1/801/files/2025/09/2025-08-20-earnings-instability.pdf) paper

## Pros + cons of each option {style="font-size: 0.8em"}


* **Dropping the observation** 
    * Does not manipulate the data values
    * But can't use that observation _at all_ in your analysis 
* **Recoding to missing**
    * Manipulates the data values 
    * Allows you to use that observation -- just not _that variable_ -- in your analysis
* **Winsorizing** 
    * Manipulates the data values 
    * Allows you to use that observation + that variable in your analysis


## `diamonds`: what would you do?
* What would you do where `x`, `y`, and `z` are all 0?
* What would you do where `y > 20`? 

## `diamonds`: what would we do?
There is often not a "right" answer or you won't know the answer without talking to a data provider. 

Our best guesses:

* Rows where `x`, `y`, and `z` are all zero: set to `NA`
* Rows where `y > 20`: winsorize? (hard to know for sure...)

## Unusual continuous values: summary

| Problem | Action |
| ---- | --- | 
| Erroneous row | drop row |
| Erroneous cell | set to NA or winsorize | 

**How do I decide which problem I have?** Examine unusual values in context of other columns (same row) and other rows (same columns). 

**How do I decide whether to set to `NA` or winsorize?** Ideally, ask your data provider what's going on with these values. 



# Unusual values case study


## Unusual values case study: roadmap {style="font-size: 0.8em"}

* Introduce `mpg` dataset
* Research question 1 

  > What is the relationship between engine size and gas mileage?"
* Research question 2

  > Why do some cars have better than typical mileage? 
* Ad hoc identification of outliers 
* Inspect fields describing outliers
* Uncover pattern

## Introducing the `mpg` dataset

- `manufacturer` — car maker (e.g., toyota, ford)  
- `model` — specific model name  
- `displ` — engine size (liters)  
- `hwy` — gas mileage highway miles per gallon  
- `class` — vehicle class (compact, suv, pickup, etc.)

## Introducing the `mpg` dataset

```{python}
mpg[["manufacturer", "model", "displ", "hwy", "class"]]
```

## Q1: What is the relationship between engine size and gas mileage?
```{.python}
base = alt.Chart(mpg).mark_point().encode(
    alt.X('displ:Q', title = "Engine size (displ)"),
    alt.Y('hwy:Q', title = "Gas mileage (hwy)")
)
base
```

## Q1: What is the relationship between engine size and gas mileage?
```{python}
#| fig-align: center
base = alt.Chart(mpg).mark_point().encode(
        alt.X('displ:Q', title = "Engine size (displ)"),
        alt.Y('hwy:Q', title = "Gas mileage (hwy)")
).properties(
    width=600, 
    height=400 )
display_altair_svg(base)
```

## Q2: Why do some cars have better than typical mileage?
```{.python .code-overflow-wrap}
potential_outliers = mpg.loc[(mpg["hwy"] > 40) 
    | ((mpg["hwy"] > 20) 
    & (mpg["displ"] > 5))]
```

```{python}
potential_outliers = mpg.loc[(mpg["hwy"] > 40) | ((mpg["hwy"] > 20) & (mpg["displ"] > 5))]
```


## Q2: Why do some cars have better than typical mileage? (plot)

```{python}
#| fig-align: center
outliers = alt.Chart(potential_outliers).mark_point(
    color='red', size = 100).encode(
        alt.X('displ:Q', title = "Engine size (displ)"),
        alt.Y('hwy:Q', title = "Gas mileage (hwy)")
        )
combined = base + outliers
display_altair_svg(combined)
```


## Q2: In-class exercise (table) {background-color="aliceblue"}

Discussion q -- which fields do you want to study further on the plot (and why?)

```{python}
print(list(potential_outliers.columns))
```

```{python}
potential_outliers
```


## Let's focus on two fields

Fields

1. `model`
2. `class`

**How did I know to use these?** Context knowledge about different types of cars. 

**Don't have context knowledge about your dataset?** Use LLM/Google/human subject matter expert to help you identify patterns


## Q2: Why do some cars have better than typical mileage?

```{python}
#| fig-align: center
labels = alt.Chart(potential_outliers).mark_text(
    align='left',
    dx=10,  # Adjust horizontal distance of text from the point
    dy=-5   # Adjust vertical distance of text from the point
).encode(
    alt.X('displ:Q', title = "Engine size (displ)"),
    alt.Y('hwy:Q', title = "Gas mileage (hwy)"),
    text='model:N'  # Display car_model as the label
).properties(
    width=600, 
    height=400)

plot = base + outliers + labels
display_altair_svg(plot)
```



## Q: How are there big engines and good mileage? `color`
```{python}
#| fig-align: center
base = alt.Chart(mpg).mark_point(size=50, shape='diamond', filled=True, opacity=1.0).encode(
    x='displ:Q',  # Quantitative variable for displacement
    y='hwy:Q',    # Quantitative variable for highway mpg
    color=alt.Color(
        'class:N',
        scale=alt.Scale(
            # range=[
            #     '#4C78A8',
            #     '#72B7B2',
            #     '#54A24B',
            #     '#9D755D',
            #     '#BAB0AC',
            #     '#B279A2',
            #     '#F1CE63',
            # ]
            scheme="category10"
        ),
        title = "Class"
    ),  # Categorical variable for class
).properties(
    width=600, 
    height=400)
plot = base+outliers
display_altair_svg(plot)
```

## Discussion {style="font-size: 0.8em"}
* We applied labels to each outlier car using the `model` field
* We hypothesized that the field `class` would capture what these models would have in common
* This is the elegant corner case where one variable (`class`) explains many of the outlier patterns in terms of fuel-efficient cars
    * All the large engines on the top-right are `class` == "2seater"
    * Both of the small engines on the top-left are `class` == "subcompact" (but many subcompacts are less fuel efficient)
* Caveat: most datasets are not as clean as this example is, but we've chosen this example for instructional purposes

## Unusual values case study: summary {style="font-size: 0.8em"}

Research question
  
> Why do some cars have better than typical mileage? (What's going on with these outliers?)

What did we do?

1. We identified outliers by hand
2. We looked at variables for those outliers
3. We went back to the plot with a variable which we thought could provide a unified explanation for the outliers.
4. It mostly did (subcompact cars and 2 seater cars are both very fuel efficient)
